# -*- coding: utf-8 -*-
"""S1-Exploring-v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AdHT6PevlqBMEBo0zkkniCcVrrQVLFfK
"""

# Install Dependencies

!pip uninstall -y numpy
!pip install --force-reinstall numpy==1.26.4

!pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 sentence-transformers==2.2.2
!pip install pandas==2.0.0
!pip install sparqlwrapper==2.0.0 transformers==4.41.0 requests==2.31.0 scikit-learn==1.2.0 pyrdf2vec
!pip install huggingface-hub==0.25.2
!pip install pyRDF2vec
!pip install rdflib==6.0.2
!pip install -U gensim
!pip install -U datasets
!pip install nlpaug
!pip install nltk==3.8.1
!pip install matplotlib seaborn

# Define Configuration and Helper Functions

import pandas as pd
import numpy as np
from SPARQLWrapper import SPARQLWrapper, JSON
import requests
import json
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import train_test_split
import time
from typing import List, Dict, Tuple
import torch
import os
from google.colab import drive
from pyrdf2vec import RDF2VecTransformer
from pyrdf2vec.embedders import Word2Vec
from pyrdf2vec.graphs import KG
from pyrdf2vec.walkers import RandomWalker
from rdflib import Graph, URIRef, Literal
import matplotlib.pyplot as plt
import seaborn as sns
from datasets import load_dataset
import nltk
import nlpaug.augmenter.word as naw

# Download NLTK data for nlpaug
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('wordnet')

# Mount Google Drive
drive.mount('/content/drive')

# Set random seed
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)

# Configuration
class Config:
    BASE_PATH = "/content/drive/MyDrive/LJMU-Datasets"
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 8
    MAX_EPOCHS = 10
    NUM_WORKERS = 0
    MAX_LENGTH = 256
    SUBSET_SIZE = 500
    WIKIDATA_SUBSET_SIZE = 30000
    HOTPOTQA_MAX_SAMPLES = 1000

CONFIG = Config()
print(f"Using device: {CONFIG.DEVICE}")

# Validate BASE_PATH
if not os.path.exists(CONFIG.BASE_PATH):
    print(f"Base path {CONFIG.BASE_PATH} does not exist. Creating it...")
    os.makedirs(CONFIG.BASE_PATH, exist_ok=True)
else:
    print(f"Base path {CONFIG.BASE_PATH} exists.")

# Create directories with _v3 postfix
for dataset in ["SQuAD_v3", "HotpotQA_v3", "Wikidata_v3"]:
    os.makedirs(os.path.join(CONFIG.BASE_PATH, dataset), exist_ok=True)

# Initialize retriever for fallback embeddings
retriever = SentenceTransformer("all-MiniLM-L6-v2", device=CONFIG.DEVICE)

# Helper Functions
def analyze_answer_lengths(df: pd.DataFrame, text_col: str = "answer") -> pd.DataFrame:
    lengths = df[text_col].apply(lambda x: len(nltk.word_tokenize(x)))
    lengths_df = pd.DataFrame(lengths, columns=["length"])
    return lengths_df.describe(), lengths_df

def balance_lengths(df: pd.DataFrame, col: str, short_threshold: int = 50, long_threshold: int = 150) -> pd.DataFrame:
    df["temp_length"] = df[col].apply(len)
    short_items = df[df["temp_length"] <= short_threshold]
    long_items = df[df["temp_length"] >= long_threshold]
    medium_items = df[(df["temp_length"] > short_threshold) & (df["temp_length"] < long_threshold)]
    target_size = min(len(short_items), len(medium_items), len(long_items))
    if target_size > 0:
        short_items = short_items.sample(n=target_size, random_state=42) if len(short_items) > target_size else short_items
        long_items = long_items.sample(n=target_size, random_state=42) if len(long_items) > target_size else long_items
        medium_items = medium_items.sample(n=target_size, random_state=42) if len(medium_items) > target_size else medium_items
    balanced_df = pd.concat([short_items, medium_items, long_items]).reset_index(drop=True)
    balanced_df = balanced_df.drop(columns=["temp_length"])
    print(f"Balanced dataset: Short={len(short_items)}, Medium={len(medium_items)}, Long={len(long_items)}")
    return balanced_df

def augment_data(df: pd.DataFrame, text_cols: List[str], aug_fraction: float = 0.3) -> pd.DataFrame:
    aug_synonym = naw.SynonymAug(aug_src='wordnet', aug_p=0.3)
    aug_dropout = naw.RandomWordAug(action="delete", aug_p=0.1)
    num_to_augment = int(len(df) * aug_fraction)
    to_augment = df[:num_to_augment].copy()
    not_to_augment = df[num_to_augment:].copy()
    for idx, row in to_augment.iterrows():
        for col in text_cols:
            text = row[col]
            aug_text = aug_synonym.augment(text)[0]
            aug_text = aug_dropout.augment(aug_text)[0]
            to_augment.at[idx, col] = aug_text
    augmented_df = pd.concat([to_augment, not_to_augment]).sample(frac=1, random_state=42).reset_index(drop=True)
    print(f"Augmented dataset size: {len(augmented_df)}")
    return augmented_df

def remove_articles(text: str) -> str:
    words = text.split()
    articles = {'a', 'an', 'the'}
    words = [word for word in words if word.lower() not in articles]
    return ' '.join(words)

def normalize_text(text: str) -> str:
    text = str(text).lower().strip()
    text = remove_articles(text)
    text = text.replace('.', '').replace(',', '').replace(';', '').replace(':', '')
    return text

print("Configuration and helper functions defined.")

# Load and Explore SQuAD Dataset

def explore_squad(squad_train, squad_val):
    print("Exploring SQuAD 2.0 dataset...")

    # Basic statistics
    print("SQuAD Train Statistics:")
    print(squad_train.describe())
    print("\nSQuAD Validation Statistics:")
    print(squad_val.describe())

    # Missing values
    print("\nMissing Values in SQuAD Train:")
    print(squad_train.isnull().sum())
    print("\nMissing Values in SQuAD Validation:")
    print(squad_val.isnull().sum())

    # Unique answers (using the 'answer' column after preprocessing)
    print("\nNumber of Unique Answers in SQuAD Train:", squad_train['answer'].nunique())
    print("Number of Unique Answers in SQuAD Validation:", squad_val['answer'].nunique())

    # Length distribution of questions and contexts (sample for visualization)
    squad_train_sample = squad_train.sample(n=min(1000, len(squad_train)), random_state=42)
    squad_val_sample = squad_val.sample(n=min(200, len(squad_val)), random_state=42)

    squad_train_sample['question_length'] = squad_train_sample['question'].apply(len)
    squad_train_sample['context_length'] = squad_train_sample['context'].apply(len)
    squad_val_sample['question_length'] = squad_val_sample['question'].apply(len)
    squad_val_sample['context_length'] = squad_val_sample['context'].apply(len)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(squad_train_sample['question_length'], bins=30, color='blue', label='Train')
    sns.histplot(squad_val_sample['question_length'], bins=30, color='orange', label='Val')
    plt.title('Question Length Distribution (SQuAD)')
    plt.xlabel('Length')
    plt.legend()

    plt.subplot(1, 2, 2)
    sns.histplot(squad_train_sample['context_length'], bins=30, color='blue', label='Train')
    sns.histplot(squad_val_sample['context_length'], bins=30, color='orange', label='Val')
    plt.title('Context Length Distribution (SQuAD)')
    plt.xlabel('Length')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Sample data
    print("\nSample SQuAD Train Data:")
    print(squad_train[['question', 'context', 'answer']].head(3))
    print("\nSample SQuAD Validation Data:")
    print(squad_val[['question', 'context', 'answer']].head(3))

print("Loading SQuAD 2.0 dataset for exploration...")
squad_dataset = load_dataset("squad_v2")

# Preprocess the dataset to extract answer text and limit size
def squad_to_df(dataset, max_samples: int):
    data = []
    for item in dataset:
        question = str(item["question"]).strip() if item["question"] else ""
        context = str(item["context"]).strip()[:1000] if item["context"] else ""
        answers = item["answers"]["text"]
        answer = str(answers[0]).strip() if answers else "unanswerable"
        if not question or not context or not answer:
            continue
        data.append({"question": question, "context": context, "answer": answer})
    df = pd.DataFrame(data)
    if len(df) > max_samples:
        df = df.sample(n=max_samples, random_state=42)
    return df

# Limit the dataset size for exploration
squad_train = squad_to_df(squad_dataset["train"], max_samples=1000)
squad_val = squad_to_df(squad_dataset["validation"], max_samples=200)

explore_squad(squad_train, squad_val)

# Process SQuAD Dataset with Enhanced Normalization

def load_squad() -> Tuple[pd.DataFrame, pd.DataFrame]:
    print("Loading SQuAD 2.0 dataset...")
    squad_dataset = load_dataset("squad_v2")
    squad_train = squad_dataset["train"]
    squad_val = squad_dataset["validation"]

    def squad_to_df(dataset, max_samples: int = CONFIG.SUBSET_SIZE) -> pd.DataFrame:
        data = []
        for item in dataset:
            question = str(item["question"]).strip() if item["question"] else ""
            context = str(item["context"]).strip()[:1000] if item["context"] else ""
            answers = item["answers"]["text"]
            answer = str(answers[0]).strip() if answers else "unanswerable"
            if not question or not context or not answer:
                continue
            data.append({"question": question, "context": context, "answer": answer})
        df = pd.DataFrame(data)
        if len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42)
        return df

    squad_df = squad_to_df(squad_train)
    squad_df = squad_df.drop_duplicates(subset=["context", "question"]).dropna()

    for col in ["question", "context", "answer"]:
        squad_df[col] = squad_df[col].apply(normalize_text)
        squad_df = squad_df[squad_df[col] != ""].reset_index(drop=True)

    squad_train_df, squad_val_df = train_test_split(squad_df, train_size=0.8, random_state=42)

    squad_train_df = balance_lengths(squad_train_df, "answer")
    squad_val_df = balance_lengths(squad_val_df, "answer")

    squad_train_df = augment_data(squad_train_df, text_cols=["question", "context"], aug_fraction=0.3)

    train_save_path = os.path.join(CONFIG.BASE_PATH, "SQuAD_v3", "squad_train.csv")
    val_save_path = os.path.join(CONFIG.BASE_PATH, "SQuAD_v3", "squad_val.csv")
    print(f"Saving SQuAD train data to: {train_save_path}")
    print(f"Saving SQuAD val data to: {val_save_path}")
    squad_train_df.to_csv(train_save_path, index=False)
    squad_val_df.to_csv(val_save_path, index=False)

    print(f"SQuAD Train Size: {len(squad_train_df)}, Val Size: {len(squad_val_df)}")
    return squad_train_df, squad_val_df

try:
    squad_train_df, squad_val_df = load_squad()
except Exception as e:
    print(f"Error in SQuAD processing: {e}")
    raise

# Explore SQuAD Dataset After Cleaning

def explore_squad_cleaned(squad_train_df, squad_val_df):
    print("Exploring SQuAD dataset after cleaning...")

    # Basic statistics
    print("SQuAD Train Statistics (After Cleaning):")
    print(squad_train_df.describe())
    print("\nSQuAD Validation Statistics (After Cleaning):")
    print(squad_val_df.describe())

    # Missing values
    print("\nMissing Values in SQuAD Train (After Cleaning):")
    print(squad_train_df.isnull().sum())
    print("\nMissing Values in SQuAD Validation (After Cleaning):")
    print(squad_val_df.isnull().sum())

    # Length distribution of questions and answers
    squad_train_df['question_length'] = squad_train_df['question'].apply(len)
    squad_train_df['answer_length'] = squad_train_df['answer'].apply(len)
    squad_val_df['question_length'] = squad_val_df['question'].apply(len)
    squad_val_df['answer_length'] = squad_val_df['answer'].apply(len)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(squad_train_df['question_length'], bins=30, color='blue', label='Train')
    sns.histplot(squad_val_df['question_length'], bins=30, color='orange', label='Val')
    plt.title('Question Length Distribution (SQuAD After Cleaning)')
    plt.xlabel('Length')
    plt.legend()

    plt.subplot(1, 2, 2)
    sns.histplot(squad_train_df['answer_length'], bins=30, color='blue', label='Train')
    sns.histplot(squad_val_df['answer_length'], bins=30, color='orange', label='Val')
    plt.title('Answer Length Distribution (SQuAD After Cleaning)')
    plt.xlabel('Length')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Sample data after cleaning
    print("\nSample SQuAD Train Data (After Cleaning):")
    print(squad_train_df[['question', 'context', 'answer']].head(3))
    print("\nSample SQuAD Validation Data (After Cleaning):")
    print(squad_val_df[['question', 'context', 'answer']].head(3))

explore_squad_cleaned(squad_train_df, squad_val_df)

# Load and Explore HotpotQA Dataset

def explore_hotpotqa(hotpotqa_train):
    print("Exploring HotpotQA dataset...")

    # Basic statistics
    print("HotpotQA Train Statistics:")
    print(hotpotqa_train.describe())

    # Missing values
    print("\nMissing Values in HotpotQA Train:")
    print(hotpotqa_train.isnull().sum())

    # Unique answers
    print("\nNumber of Unique Answers in HotpotQA Train:", hotpotqa_train['answer'].nunique())

    # Length distribution of questions and contexts (sample for visualization)
    hotpotqa_train_sample = hotpotqa_train.sample(n=min(1000, len(hotpotqa_train)), random_state=42)
    hotpotqa_train_sample['question_length'] = hotpotqa_train_sample['question'].apply(len)
    hotpotqa_train_sample['context_length'] = hotpotqa_train_sample['context'].apply(len)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(hotpotqa_train_sample['question_length'], bins=30, color='blue', label='Train')
    plt.title('Question Length Distribution (HotpotQA)')
    plt.xlabel('Length')
    plt.legend()

    plt.subplot(1, 2, 2)
    sns.histplot(hotpotqa_train_sample['context_length'], bins=30, color='blue', label='Train')
    plt.title('Context Length Distribution (HotpotQA)')
    plt.xlabel('Length')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Sample data
    print("\nSample HotpotQA Train Data:")
    print(hotpotqa_train[['question', 'context', 'answer']].head(3))

print("Loading HotpotQA dataset for exploration...")
hotpotqa_dataset = load_dataset("hotpot_qa", "fullwiki")

# Preprocess the dataset to limit size
def hotpotqa_to_df(dataset, max_samples: int = 1000):
    data = []
    for item in dataset:
        question = str(item["question"]).strip() if item["question"] else ""
        context = " ".join([str(para[1]) for para in item["context"]])[:1000] if item["context"] else ""
        answer = str(item["answer"]).strip() if item["answer"] else ""
        if not question or not context or not answer:
            continue
        data.append({"question": question, "context": context, "answer": answer})
    df = pd.DataFrame(data)
    if len(df) > max_samples:
        df = df.sample(n=max_samples, random_state=42)
    return df

hotpotqa_train = hotpotqa_to_df(hotpotqa_dataset["train"], max_samples=1000)
explore_hotpotqa(hotpotqa_train)

# Process HotpotQA Dataset with Enhanced Normalization

def load_hotpotqa(max_samples: int = CONFIG.HOTPOTQA_MAX_SAMPLES) -> pd.DataFrame:
    print("Loading HotpotQA dataset...")
    hotpotqa_dataset = load_dataset("hotpot_qa", "fullwiki")
    hotpotqa_train = hotpotqa_dataset["train"]

    def hotpotqa_to_df(dataset, max_samples: int) -> pd.DataFrame:
        data = []
        for item in dataset:
            question = str(item["question"]).strip() if item["question"] else ""
            context = " ".join([str(para[1]) for para in item["context"]])[:1000] if item["context"] else ""
            answer = str(item["answer"]).strip() if item["answer"] else ""
            if not question or not context or not answer:
                continue
            data.append({"question": question, "context": context, "answer": answer})
        df = pd.DataFrame(data)
        if len(df) > max_samples:
            df = df.sample(n=max_samples, random_state=42)
        return df

    hotpotqa_train_df = hotpotqa_to_df(hotpotqa_train, max_samples)
    hotpotqa_train_df = hotpotqa_train_df.drop_duplicates(subset=["context", "question"]).dropna()

    for col in ["question", "context", "answer"]:
        hotpotqa_train_df[col] = hotpotqa_train_df[col].apply(normalize_text)
        hotpotqa_train_df = hotpotqa_train_df[hotpotqa_train_df[col] != ""].reset_index(drop=True)

    hotpotqa_train_df = balance_lengths(hotpotqa_train_df, "answer")

    hotpotqa_train_df = augment_data(hotpotqa_train_df, text_cols=["question", "context"], aug_fraction=0.3)

    save_path = os.path.join(CONFIG.BASE_PATH, "HotpotQA_v3", "hotpotqa_train.csv")
    print(f"Saving HotpotQA train data to: {save_path}")
    hotpotqa_train_df.to_csv(save_path, index=False)

    print(f"HotpotQA Train Size: {len(hotpotqa_train_df)}")
    return hotpotqa_train_df

try:
    hotpotqa_train_df = load_hotpotqa()
except Exception as e:
    print(f"Error in HotpotQA processing: {e}")
    raise

# Explore HotpotQA Dataset After Cleaning

def explore_hotpotqa_cleaned(hotpotqa_train_df):
    print("Exploring HotpotQA dataset after cleaning...")

    # Basic statistics
    print("HotpotQA Train Statistics (After Cleaning):")
    print(hotpotqa_train_df.describe())

    # Missing values
    print("\nMissing Values in HotpotQA Train (After Cleaning):")
    print(hotpotqa_train_df.isnull().sum())

    # Length distribution of questions and answers
    hotpotqa_train_df['question_length'] = hotpotqa_train_df['question'].apply(len)
    hotpotqa_train_df['answer_length'] = hotpotqa_train_df['answer'].apply(len)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(hotpotqa_train_df['question_length'], bins=30, color='blue', label='Train')
    plt.title('Question Length Distribution (HotpotQA After Cleaning)')
    plt.xlabel('Length')
    plt.legend()

    plt.subplot(1, 2, 2)
    sns.histplot(hotpotqa_train_df['answer_length'], bins=30, color='blue', label='Train')
    plt.title('Answer Length Distribution (HotpotQA After Cleaning)')
    plt.xlabel('Length')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # Sample data after cleaning
    print("\nSample HotpotQA Train Data (After Cleaning):")
    print(hotpotqa_train_df[['question', 'context', 'answer']].head(3))

explore_hotpotqa_cleaned(hotpotqa_train_df)

# Load and Explore Wikidata Dataset

WIKIDATA_ENDPOINT = "https://query.wikidata.org/sparql"

def fetch_wikidata_triples(limit: int = CONFIG.WIKIDATA_SUBSET_SIZE, batch_size: int = 5000) -> pd.DataFrame:
    sparql = SPARQLWrapper(WIKIDATA_ENDPOINT)
    sparql.setReturnFormat(JSON)
    triples = []
    offset = 0
    remaining = limit

    while remaining > 0:
        current_limit = min(batch_size, remaining)
        # Simplified SPARQL query to increase the likelihood of results
        query = """
        SELECT ?subject ?subjectLabel ?predicate ?object ?objectLabel WHERE {{
          {{ ?subject wdt:P31 wd:Q4830453; wdt:P159 ?object.
            ?subject rdfs:label ?subjectLabel. ?object rdfs:label ?objectLabel.
            FILTER(LANG(?subjectLabel) = "en" || LANG(?subjectLabel) = "")
            FILTER(LANG(?objectLabel) = "en" || LANG(?objectLabel) = "") }}
          UNION
          {{ ?subject wdt:P31 wd:Q12136; wdt:P780 ?object.
            ?subject rdfs:label ?subjectLabel. ?object rdfs:label ?objectLabel.
            FILTER(LANG(?subjectLabel) = "en" || LANG(?subjectLabel) = "")
            FILTER(LANG(?objectLabel) = "en" || LANG(?objectLabel) = "") }}
        }}
        LIMIT {} OFFSET {}
        """.format(current_limit, offset)
        try:
            sparql.setQuery(query)
            results = sparql.query().convert()
            batch_triples = [
                {
                    "subject": r["subjectLabel"]["value"],
                    "predicate": "headquarters" if "P159" in r["subject"]["value"] else "symptoms",
                    "object": r["objectLabel"]["value"],
                    "subject_uri": r["subject"]["value"],
                    "object_uri": r["object"]["value"]
                }
                for r in results["results"]["bindings"]
            ]
            triples.extend(batch_triples)
            print(f"Fetched {len(batch_triples)} triples at offset {offset}")
            offset += current_limit
            remaining -= current_limit
            if len(batch_triples) < current_limit:
                break
        except Exception as e:
            print(f"Wikidata fetch failed at offset {offset}: {e}")
            break

    df = pd.DataFrame(triples)
    if df.empty:
        # Fallback data if query fails
        print("No triples fetched from Wikidata. Using fallback data...")
        fallback_data = [
            {"subject": "Bank of America", "predicate": "headquarters", "object": "Charlotte", "subject_uri": "http://www.wikidata.org/entity/Q16565", "object_uri": "http://www.wikidata.org/entity/Q16566"},
            {"subject": "Diabetes", "predicate": "symptoms", "object": "Fatigue", "subject_uri": "http://www.wikidata.org/entity/Q12136", "object_uri": "http://www.wikidata.org/entity/Q12137"}
        ]
        df = pd.DataFrame(fallback_data)
    print(f"Fetched {len(df)} Wikidata triples in total")
    return df

def explore_wikidata(wikidata_raw):
    print("Exploring Wikidata dataset...")

    # Basic statistics
    print("Wikidata Raw Statistics:")
    print(wikidata_raw.describe())

    # Missing values
    print("\nMissing Values in Wikidata Raw:")
    print(wikidata_raw.isnull().sum())

    # Unique subjects and objects
    print("\nNumber of Unique Subjects in Wikidata:", wikidata_raw['subject'].nunique())
    print("Number of Unique Objects in Wikidata:", wikidata_raw['object'].nunique())

    # Predicate distribution (sample for visualization)
    wikidata_raw_sample = wikidata_raw.sample(n=min(1000, len(wikidata_raw)), random_state=42)
    plt.figure(figsize=(8, 4))
    sns.countplot(data=wikidata_raw_sample, x='predicate', hue='predicate')
    plt.title('Predicate Distribution in Wikidata (Finance and Healthcare)')
    plt.xlabel('Predicate')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()

    # Sample data
    print("\nSample Wikidata Raw Data:")
    print(wikidata_raw[['subject', 'predicate', 'object']].head(3))

print("Loading Wikidata dataset for exploration...")
# Limit the number of triples fetched for exploration
wikidata_raw = fetch_wikidata_triples(limit=1000)  # Reduced from CONFIG.WIKIDATA_SUBSET_SIZE (30,000)
explore_wikidata(wikidata_raw)

# Process Wikidata Dataset with Enhanced Context, Normalization, and Increased Size

WIKIDATA_ENDPOINT = "https://query.wikidata.org/sparql"

def fetch_wikidata_triples(limit: int = CONFIG.WIKIDATA_SUBSET_SIZE, batch_size: int = 5000) -> pd.DataFrame:
    sparql = SPARQLWrapper(WIKIDATA_ENDPOINT)
    sparql.setReturnFormat(JSON)
    triples = []
    offset = 0
    remaining = limit

    while remaining > 0:
        current_limit = min(batch_size, remaining)
        query = """
        SELECT ?subject ?subjectLabel ?predicate ?object ?objectLabel WHERE {{
          {{ ?subject wdt:P31 wd:Q4830453; wdt:P159 ?object.
            ?subject rdfs:label ?subjectLabel. ?object rdfs:label ?objectLabel.
            FILTER(LANG(?subjectLabel) = "en" || LANG(?subjectLabel) = "")
            FILTER(LANG(?objectLabel) = "en" || LANG(?objectLabel) = "") }}
          UNION
          {{ ?subject wdt:P31 wd:Q12136; wdt:P780 ?object.
            ?subject rdfs:label ?subjectLabel. ?object rdfs:label ?objectLabel.
            FILTER(LANG(?subjectLabel) = "en" || LANG(?subjectLabel) = "")
            FILTER(LANG(?objectLabel) = "en" || LANG(?objectLabel) = "") }}
        }}
        LIMIT {} OFFSET {}
        """.format(current_limit, offset)
        try:
            sparql.setQuery(query)
            results = sparql.query().convert()
            batch_triples = [
                {
                    "subject": r["subjectLabel"]["value"],
                    "predicate": "headquarters" if "P159" in r["subject"]["value"] else "symptoms",
                    "object": r["objectLabel"]["value"],
                    "subject_uri": r["subject"]["value"],
                    "object_uri": r["object"]["value"]
                }
                for r in results["results"]["bindings"]
            ]
            triples.extend(batch_triples)
            print(f"Fetched {len(batch_triples)} triples at offset {offset}")
            offset += current_limit
            remaining -= current_limit
            if len(batch_triples) < current_limit:
                break
        except Exception as e:
            print(f"Wikidata fetch failed at offset {offset}: {e}")
            break

    df = pd.DataFrame(triples)
    if df.empty:
        print("No triples fetched from Wikidata. Using fallback data...")
        fallback_data = [
            {"subject": "Bank of America", "predicate": "headquarters", "object": "Charlotte", "subject_uri": "http://www.wikidata.org/entity/Q16565", "object_uri": "http://www.wikidata.org/entity/Q16566"},
            {"subject": "Diabetes", "predicate": "symptoms", "object": "Fatigue", "subject_uri": "http://www.wikidata.org/entity/Q12136", "object_uri": "http://www.wikidata.org/entity/Q12137"}
        ]
        df = pd.DataFrame(fallback_data)
    print(f"Fetched {len(df)} Wikidata triples in total")
    return df

def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    df = df.drop_duplicates().dropna().reset_index(drop=True)
    print(f"Cleaned DataFrame to {len(df)} rows")
    return df

def normalize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    for col in df.columns:
        if df[col].dtype == "object" and col not in ["subject_uri", "object_uri"]:
            df[col] = df[col].apply(normalize_text)
            if col == "subject":
                df[col] = df[col].replace({"bank of america corp": "bank of america"})
    print("Normalization completed")
    return df

def generate_rdf2vec_embeddings(triples_df: pd.DataFrame) -> pd.DataFrame:
    start_time = time.time()
    entities = list(set(triples_df["subject_uri"].tolist() + triples_df["object_uri"].tolist()))
    triples = [(row["subject_uri"], row["predicate"], row["object_uri"]) for _, row in triples_df.iterrows()]

    g = Graph()
    for s, p, o in triples:
        subject = URIRef(s)
        predicate = URIRef(p) if p.startswith("http") else Literal(p)
        obj = URIRef(o)
        g.add((subject, predicate, obj))

    embedder = Word2Vec(vector_size=200, min_count=1)
    transformer = RDF2VecTransformer(
        walkers=[RandomWalker(max_walks=200, max_depth=4, random_state=42)],
        embedder=embedder
    )

    try:
        embeddings = transformer.fit_transform(kg=g, entities=entities)
        emb_dict = dict(zip(entities, embeddings))
        triple_embeddings = [
            (emb_dict[row["subject_uri"]] + emb_dict[row["object_uri"]]) / 2
            for _, row in triples_df.iterrows()
        ]
        triples_df["embedding"] = triple_embeddings
        print(f"Generated RDF2Vec embeddings in {time.time() - start_time:.2f} seconds")
    except Exception as e:
        print(f"RDF2Vec failed: {e}, falling back to SentenceTransformer")
        triple_texts = triples_df.apply(lambda row: f"{row['subject']} {row['predicate']} {row['object']}", axis=1).tolist()
        embeddings = retriever.encode(triple_texts, batch_size=32, show_progress_bar=True)
        triples_df["embedding"] = list(embeddings)
    return triples_df

def load_wikidata(max_samples: int = CONFIG.WIKIDATA_SUBSET_SIZE) -> pd.DataFrame:
    print("Loading Wikidata dataset...")
    wikidata_df = fetch_wikidata_triples(limit=max_samples)

    wikidata_df = clean_dataframe(wikidata_df)
    wikidata_df = normalize_dataframe(wikidata_df)

    wikidata_df["context"] = wikidata_df.apply(
        lambda row: f"{row['subject']}, {row['predicate']} related entity, has {row['predicate']} of {row['object']}, which is common in such cases.",
        axis=1
    )
    print("Added enhanced synthetic context to Wikidata triples")

    wikidata_df["question"] = wikidata_df.apply(lambda row: f"{row['subject']} {row['predicate']}", axis=1)
    wikidata_df["answer"] = wikidata_df["object"]

    wikidata_df["triple_text"] = wikidata_df.apply(lambda row: f"{row['subject']} {row['predicate']} {row['object']}", axis=1)
    wikidata_df = balance_lengths(wikidata_df, "triple_text")

    wikidata_df = augment_data(wikidata_df, text_cols=["subject", "object"], aug_fraction=0.5)

    wikidata_df = generate_rdf2vec_embeddings(wikidata_df)

    wikidata_df["triple_length"] = wikidata_df["triple_text"].apply(len)
    wikidata_df = wikidata_df[wikidata_df["triple_length"] <= 100].reset_index(drop=True)

    wikidata_save_path = os.path.join(CONFIG.BASE_PATH, "Wikidata_v3", "wikidata_preprocessed_v3.pkl")
    triples_save_path = os.path.join(CONFIG.BASE_PATH, "Wikidata_v3", "wikidata_triples.tsv")
    print(f"Saving Wikidata DataFrame to: {wikidata_save_path}")
    print(f"Saving Wikidata triples to: {triples_save_path}")
    wikidata_df.to_pickle(wikidata_save_path)
    with open(triples_save_path, "w") as f:
        for _, row in wikidata_df.iterrows():
            f.write(f"{row['subject']}\t{row['predicate']}\t{row['object']}\n")

    print(f"Wikidata Size: {len(wikidata_df)}")
    return wikidata_df

try:
    wikidata_df = load_wikidata()
except Exception as e:
    print(f"Error in Wikidata processing: {e}")
    raise

# Explore Wikidata Dataset After Cleaning

def explore_wikidata_cleaned(wikidata_df):
    print("Exploring Wikidata dataset after cleaning...")

    # Basic statistics
    print("Wikidata Statistics (After Cleaning):")
    print(wikidata_df.describe())

    # Missing values
    print("\nMissing Values in Wikidata (After Cleaning):")
    print(wikidata_df.isnull().sum())

    # Predicate distribution after cleaning (sample for visualization)
    wikidata_df_sample = wikidata_df.sample(n=min(1000, len(wikidata_df)), random_state=42)
    plt.figure(figsize=(8, 4))
    sns.countplot(data=wikidata_df_sample, x='predicate', hue='predicate')
    plt.title('Predicate Distribution in Wikidata (After Cleaning)')
    plt.xlabel('Predicate')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()

    # Triple length distribution (sample for visualization)
    plt.figure(figsize=(8, 4))
    sns.histplot(wikidata_df_sample['triple_length'], bins=30, color='green')
    plt.title('Triple Length Distribution (Wikidata After Cleaning)')
    plt.xlabel('Length')
    plt.ylabel('Count')
    plt.show()

    # Sample data after cleaning
    print("\nSample Wikidata Data (After Cleaning):")
    print(wikidata_df[['question', 'context', 'answer']].head(3))

explore_wikidata_cleaned(wikidata_df)

# Combine QA Datasets and Save Separately

def combine_qa_datasets(squad_train_df: pd.DataFrame, hotpotqa_train_df: pd.DataFrame, squad_val_df: pd.DataFrame) -> None:
    print("Combining QA datasets (SQuAD and HotpotQA)...")

    qa_train_df = pd.concat([squad_train_df[["question", "context", "answer"]],
                             hotpotqa_train_df[["question", "context", "answer"]]], ignore_index=True)

    qa_val_df = squad_val_df[["question", "context", "answer"]]

    triple_train_df = wikidata_df[["question", "context", "answer"]]

    qa_train_path = os.path.join(CONFIG.BASE_PATH, "qa_train_v3.csv")
    qa_val_path = os.path.join(CONFIG.BASE_PATH, "qa_val_v3.csv")
    triple_train_path = os.path.join(CONFIG.BASE_PATH, "triple_train_v3.csv")

    print(f"Saving QA train dataset to: {qa_train_path}")
    qa_train_df.to_csv(qa_train_path, index=False)
    print(f"Saving QA val dataset to: {qa_val_path}")
    qa_val_df.to_csv(qa_val_path, index=False)
    print(f"Saving triple train dataset to: {triple_train_path}")
    triple_train_df.to_csv(triple_train_path, index=False)

    print(f"QA Train Size: {len(qa_train_df)}, QA Val Size: {len(qa_val_df)}, Triple Train Size: {len(triple_train_df)}")

try:
    combine_qa_datasets(squad_train_df, hotpotqa_train_df, squad_val_df)
except Exception as e:
    print(f"Error in combining QA datasets: {e}")
    raise

# Create Checkpoint for Step 1

checkpoint = {
    "step": "step1_data_collection",
    "squad_subset_size": CONFIG.SUBSET_SIZE,
    "squad_train_size": len(squad_train_df),
    "squad_val_size": len(squad_val_df),
    "squad_answer_length_distribution_train": analyze_answer_lengths(squad_train_df, "answer")[0].to_dict(),
    "squad_answer_length_distribution_val": analyze_answer_lengths(squad_val_df, "answer")[0].to_dict(),
    "hotpotqa_max_samples": CONFIG.HOTPOTQA_MAX_SAMPLES,
    "hotpotqa_train_size": len(hotpotqa_train_df),
    "hotpotqa_answer_length_distribution": analyze_answer_lengths(hotpotqa_train_df, "answer")[0].to_dict(),
    "wikidata_subset_size": CONFIG.WIKIDATA_SUBSET_SIZE,
    "wikidata_size": len(wikidata_df),
    "wikidata_triple_length_distribution": analyze_answer_lengths(wikidata_df, "triple_text")[0].to_dict(),
    "augmentation_applied": True,
    "augmentation_fraction": {"qa": 0.3, "triple": 0.5},
    "datasets_paths": {
        "qa_train": os.path.join(CONFIG.BASE_PATH, "qa_train_v3.csv"),
        "qa_val": os.path.join(CONFIG.BASE_PATH, "qa_val_v3.csv"),
        "triple_train": os.path.join(CONFIG.BASE_PATH, "triple_train_v3.csv"),
        "wikidata_preprocessed": os.path.join(CONFIG.BASE_PATH, "Wikidata_v3", "wikidata_preprocessed_v3.pkl"),
        "wikidata_triples": os.path.join(CONFIG.BASE_PATH, "Wikidata_v3", "wikidata_triples.tsv")
    }
}

checkpoint_path = os.path.join(CONFIG.BASE_PATH, "step1_checkpoint_v3.json")
with open(checkpoint_path, "w") as f:
    json.dump(checkpoint, f)

print(f"Checkpoint saved at: {checkpoint_path}")